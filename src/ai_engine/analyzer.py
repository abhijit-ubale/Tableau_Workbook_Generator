"""
AI Analysis Engine for Tableau Dashboard Generation.
Uses AzureOpenAI and Langchain for intelligent data analysis and dashboard recommendations.
Supports advanced Tableau calculations, including table calculations and Level of Detail (LOD) expressions.
"""

import json
import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime

from langchain_openai import AzureChatOpenAI
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain.schema import BaseOutputParser
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

from ..models.schemas import (
    DatasetSchema, AIAnalysisRequest, AIAnalysisResponse, 
    AIRecommendation, KPISpecification, VisualizationSpec,
    VisualizationType, ColorScheme, DataType
)
from ..models.schemas import CalculatedFieldSpec
from ..utils.config import Config
from ..utils.logger import get_logger

logger = get_logger(__name__)

class DataInsights(BaseModel):
    """
    Structured container for AI-generated data insights and analysis results.
    
    This Pydantic BaseModel encapsulates comprehensive insights about a dataset,
    generated by the AI analysis engine. It provides structured output from the
    data analysis chain, categorizing findings into data characteristics, business
    potential, quality issues, and preprocessing recommendations.
    
    This class serves as the primary output model for the data analyzer chain,
    ensuring type-safe and validated data flow through the analysis pipeline.
    
    Attributes
    ----------
    data_characteristics : Dict[str, Any]
        Dictionary containing key statistical and structural characteristics of the dataset.
        
        Common keys include:
        - 'total_rows': Number of records in dataset
        - 'total_columns': Number of features/fields
        - 'numeric_columns': Count of numeric fields
        - 'categorical_columns': Count of categorical fields
        - 'temporal_columns': Count of date/time fields
        - 'memory_size': Estimated memory footprint
        - 'sparsity': Proportion of missing values
        - 'cardinality': Range of unique value counts per column
        - 'dominant_patterns': Identified recurring patterns
        
        Type: Dict[str, Any]
        Example: {
            'total_rows': 50000,
            'total_columns': 15,
            'numeric_columns': 8,
            'categorical_columns': 7,
            'memory_size': '125MB',
            'dominant_patterns': ['Time series', 'Hierarchical relationships']
        }
    
    business_potential : List[str]
        List of potential business insights and analytical opportunities identified
        in the dataset.
        
        These are actionable insights that could be visualized or monitored in a
        dashboard, ranked by business relevance and impact potential.
        
        Type: List[str]
        Example: [
            'Track revenue trends by product category and region',
            'Identify high-value customer segments for targeted marketing',
            'Monitor inventory turnover rates by warehouse location',
            'Analyze seasonal demand patterns for supply chain optimization'
        ]
    
    data_quality_issues : List[str]
        List of identified data quality problems that may impact dashboard accuracy
        and reliability.
        
        Issues are identified during AI analysis and include items such as:
        - Missing values and null counts
        - Outliers and anomalies
        - Inconsistent formatting
        - Duplicate records
        - Type mismatches
        - Temporal gaps or inconsistencies
        - Reference integrity issues
        
        Type: List[str]
        Example: [
            '15% missing values in "Commission" column',
            'Date field "CreatedAt" contains 2,000 future dates',
            '500 duplicate customer records identified',
            'Revenue values range from -$50K to $5M (check for data entry errors)'
        ]
    
    recommended_preprocessing : List[str]
        List of recommended data preprocessing and cleaning steps to improve
        data quality before dashboard generation.
        
        Recommendations are prioritized and actionable, providing guidance on:
        - Handling missing values (imputation, deletion)
        - Outlier treatment strategies
        - Data normalization and scaling
        - Type conversions
        - Deduplication approaches
        - Feature engineering suggestions
        - Temporal alignment
        - Aggregation strategies
        
        Type: List[str]
        Example: [
            'Impute missing Commission values using mean by product category',
            'Remove future-dated records from CreatedAt field (validation error)',
            'Consolidate duplicate customer records by matching on email + phone',
            'Apply log transformation to Revenue for better visualization scaling',
            'Standardize date format across all temporal columns'
        ]
    
    Notes
    -----
    This class is used as output from the `_run_data_analysis()` method and serves
    as input to downstream analysis chains (dashboard designer, KPI generator).
    
    Validation:
    - All fields are required (use Ellipsis in Field definitions)
    - Lists must contain at least one item (typically enforced by AI prompt)
    - Dictionary values can be heterogeneous (for flexibility in characteristics)
    - Empty lists should be avoided; use sensible defaults instead
    
    JSON Serialization:
    The model is JSON-serializable via the `.dict()` method, making it suitable
    for passing to subsequent LangChain chains as formatted input.
    
    Error Handling:
    The `DataInsightsOutputParser` in `_create_data_analyzer_chain()` provides
    fallback values if AI response parsing fails, ensuring the analysis pipeline
    continues even if data insights cannot be generated.
    
    Examples
    --------
    Creating a DataInsights instance directly:
    
    >>> insights = DataInsights(
    ...     data_characteristics={
    ...         'total_rows': 10000,
    ...         'total_columns': 25,
    ...         'numeric_columns': 12,
    ...         'memory_size': '50MB'
    ...     },
    ...     business_potential=[
    ...         'Analyze sales trends',
    ...         'Customer segmentation',
    ...         'Forecast demand'
    ...     ],
    ...     data_quality_issues=['5% missing in date field'],
    ...     recommended_preprocessing=[
    ...         'Handle missing dates',
    ...         'Remove duplicates'
    ...     ]
    ... )
    
    Accessing insights in downstream processing:
    
    >>> async def process_insights(insights: DataInsights):
    ...     characteristics = insights.data_characteristics
    ...     potential = ", ".join(insights.business_potential)
    ...     issues = len(insights.data_quality_issues)
    ...     print(f"Found {issues} quality issues")
    ...     print(f"Business opportunities: {potential}")
    
    Converting to JSON for AI chains:
    
    >>> insights_json = insights.dict()
    >>> formatted = json.dumps(insights_json, indent=2)
    >>> # Pass to downstream LangChain chain
    
    See Also
    --------
    AIAnalysisResponse : Container for complete analysis results
    _run_data_analysis : Method that generates DataInsights
    _create_data_analyzer_chain : Method that returns DataInsights output
    
    Field Descriptions
    ------------------
    Each field uses Pydantic's Field descriptor with descriptive text:
    
    - `data_characteristics`: Quantitative and structural properties of dataset
    - `business_potential`: Qualitative business insights and opportunities
    - `data_quality_issues`: Problems that need attention or remediation
    - `recommended_preprocessing`: Actions to improve data for dashboard creation
    """
    
    data_characteristics: Dict[str, Any] = Field(
        ..., 
        description="Key characteristics of the dataset including row/column counts, data types, and statistical properties"
    )
    business_potential: List[str] = Field(
        ..., 
        description="Potential business insights and analytical opportunities identified in the data"
    )
    data_quality_issues: List[str] = Field(
        ..., 
        description="Identified data quality problems that may impact dashboard accuracy and reliability"
    )
    recommended_preprocessing: List[str] = Field(
        ..., 
        description="Recommended data preprocessing and cleaning steps to improve data quality before dashboard generation"
    )



class TableauDashboardAnalyzer:
    """
    Advanced AI analyzer for Tableau dashboard generation using meta-prompting techniques.
    
    This class orchestrates multiple AI chains to perform comprehensive analysis of datasets
    and generate intelligent dashboard recommendations including visualizations, KPIs, and
    design specifications. It leverages Azure OpenAI with meta-prompting for enhanced
    accuracy and consistency.
    
    Attributes
    ----------
    config : Config
        Application configuration object containing Azure OpenAI credentials and settings
    llm : AzureChatOpenAI
        Initialized Azure OpenAI language model instance
    data_analyzer_chain : LangChain Chain
        Chain for analyzing dataset characteristics and business potential
    dashboard_designer_chain : LangChain Chain
        Chain for generating dashboard design recommendations
    kpi_generator_chain : LangChain Chain
        Chain for creating KPI specifications with Tableau calculations
    visualization_recommender_chain : LangChain Chain
        Chain for recommending optimal visualizations
    
    Examples
    --------
    >>> config = get_config()
    >>> analyzer = TableauDashboardAnalyzer(config)
    >>> request = AIAnalysisRequest(...)
    >>> response = await analyzer.analyze_dataset(request)
    """
    
    def __init__(self, config: Config):
        """
        Initialize the Tableau Dashboard Analyzer.
        
        Parameters
        ----------
        config : Config
            Application configuration object with Azure OpenAI settings
            
        Raises
        ------
        Exception
            If Azure OpenAI initialization fails
            
        Notes
        -----
        Initializes all AI chains (data analyzer, dashboard designer, KPI generator,
        and visualization recommender) required for comprehensive analysis.
        """
        self.config = config
        self.llm = self._initialize_llm()
        self.data_analyzer_chain = self._create_data_analyzer_chain()
        self.dashboard_designer_chain = self._create_dashboard_designer_chain()
        self.kpi_generator_chain = self._create_kpi_generator_chain()
        self.visualization_recommender_chain = self._create_visualization_recommender_chain()
    
    def _initialize_llm(self) -> AzureChatOpenAI:
        """
        Initialize the Azure OpenAI language model.
        
        Creates and configures an AzureChatOpenAI instance using credentials and
        parameters from the application configuration.
        
        Returns
        -------
        AzureChatOpenAI
            Configured Azure OpenAI language model instance
            
        Raises
        ------
        Exception
            If Azure OpenAI endpoint, API key, or other credentials are invalid or missing
            
        Notes
        -----
        Uses configuration parameters:
        - azure_openai.endpoint: Azure OpenAI resource endpoint
        - azure_openai.api_key: Azure OpenAI API key
        - azure_openai.api_version: Azure OpenAI API version
        - azure_openai.deployment_name: Deployment name
        - azure_openai.model_name: Model identifier
        - azure_openai.temperature: Sampling temperature (0.0-2.0)
        - azure_openai.max_tokens: Maximum response length
        - azure_openai.top_p: Nucleus sampling parameter
        """
        try:
            return AzureChatOpenAI(
                azure_endpoint=self.config.azure_openai.endpoint,
                api_key=self.config.azure_openai.api_key,
                api_version=self.config.azure_openai.api_version,
                deployment_name=self.config.azure_openai.deployment_name,
                model_name=self.config.azure_openai.model_name,
                temperature=self.config.azure_openai.temperature,
                max_tokens=self.config.azure_openai.max_tokens,
                top_p=self.config.azure_openai.top_p
            )
        except Exception as e:
            logger.error(f"Failed to initialize Azure OpenAI: {e}")
            raise
    
    def _create_data_analyzer_chain(self):
        """
        Create the data analysis chain with meta-prompting.
        
        Constructs a LangChain pipeline that analyzes dataset schemas and provides
        structured insights about data characteristics, business potential, quality
        issues, and preprocessing recommendations.
        
        Returns
        -------
        LangChain Chain
            Configured chain with system/human prompts and output parser for
            DataInsights structured output
            
        Notes
        -----
        Chain flow:
        1. Analyzes dataset schema and column statistics
        2. Identifies business potential and insights
        3. Detects data quality issues
        4. Recommends preprocessing steps
        
        Uses meta-prompting from config.meta_prompting.system_prompts.data_analyzer
        with critical instructions for consistent, high-quality analysis.
        """
        system_prompt = SystemMessagePromptTemplate.from_template(
            self.config.meta_prompting.system_prompts.data_analyzer + 
            """
            
            CRITICAL INSTRUCTIONS:
            1. Analyze the dataset schema and provide structured insights
            2. Focus on business relevance and dashboard potential
            3. Identify key metrics, dimensions, and relationships
            4. Consider data quality and preprocessing needs
            5. Provide actionable recommendations
            
            Response must be a valid JSON object with these keys:
            - data_characteristics: Object with key dataset properties
            - business_potential: Array of potential business insights
            - data_quality_issues: Array of identified issues
            - recommended_preprocessing: Array of preprocessing suggestions
            """
        )
        
        human_prompt = HumanMessagePromptTemplate.from_template(
            """
            Analyze this dataset schema for Tableau dashboard creation:
            
            Dataset: {dataset_name}
            Rows: {total_rows}
            Columns: {total_columns}
            Data Quality Score: {data_quality_score}
            
            Column Details:
            {column_details}
            
            Business Context: {business_context}
            Business Goals: {business_goals}
            Target Audience: {target_audience}
            
            Provide comprehensive analysis as requested.
            """
        )
        
        prompt_template = ChatPromptTemplate.from_messages([system_prompt, human_prompt])
        
        class DataInsightsOutputParser(BaseOutputParser[DataInsights]):
            """Parser for converting AI response to DataInsights object."""
            
            def parse(self, text: str) -> DataInsights:
                """
                Parse AI response text into DataInsights object.
                
                Parameters
                ----------
                text : str
                    Raw AI response text, potentially containing markdown formatting
                    
                Returns
                -------
                DataInsights
                    Parsed data insights with fallback to default values on error
                """
                try:
                    # Clean the response to extract JSON
                    cleaned_text = text.strip()
                    if cleaned_text.startswith("```json"):
                        cleaned_text = cleaned_text[7:-3]
                    elif cleaned_text.startswith("```"):
                        cleaned_text = cleaned_text[3:-3]
                    
                    data = json.loads(cleaned_text)
                    return DataInsights(**data)
                except (json.JSONDecodeError, ValueError) as e:
                    logger.warning(f"Failed to parse AI response as JSON: {e}")
                    # Fallback parsing
                    return DataInsights(
                        data_characteristics={"parsed_error": str(e)},
                        business_potential=["Unable to parse AI response"],
                        data_quality_issues=["Response parsing failed"],
                        recommended_preprocessing=["Review AI response format"]
                    )
        
        return prompt_template | self.llm | DataInsightsOutputParser()
    
    def _create_dashboard_designer_chain(self):
        """
        Create dashboard design recommendation chain.
        
        Constructs a LangChain pipeline that generates dashboard layout, color scheme,
        and performance optimization recommendations based on data analysis and
        business goals.
        
        Returns
        -------
        LangChain Chain
            Configured chain for dashboard design recommendations
            
        Notes
        -----
        Recommendations include:
        - Layout optimization (grid, free-form, etc.)
        - Color scheme selection with confidence scores
        - Performance considerations for large datasets
        - Alternative options for each recommendation
        
        Uses meta-prompting from config.meta_prompting.system_prompts.dashboard_designer
        """
        system_prompt = SystemMessagePromptTemplate.from_template(
            self.config.meta_prompting.system_prompts.dashboard_designer + 
            """
            
            CRITICAL INSTRUCTIONS:
            1. Design optimal dashboard layouts based on data analysis
            2. Consider user experience and business goals
            3. Recommend color schemes and visual hierarchy
            4. Focus on performance and usability
            5. Provide reasoning for all recommendations
            
            Response format (JSON):
            {
              "layout_recommendation": {
                "confidence_score": 0.85,
                "reasoning": "Explanation of layout choice",
                "alternatives": ["Alternative 1", "Alternative 2"]
              },
              "color_scheme_recommendation": {
                "confidence_score": 0.90,
                "reasoning": "Color scheme rationale", 
                "alternatives": ["tableau20", "category10"]
              },
              "performance_considerations": ["Consideration 1", "Consideration 2"]
            }
            """
        )
        
        human_prompt = HumanMessagePromptTemplate.from_template(
            """
            Design dashboard layout based on:
            
            Data Insights: {data_insights}
            Business Goals: {business_goals}
            Target Audience: {target_audience}
            Dataset Schema: {dataset_info}
            
            Provide structured design recommendations.
            """
        )
        
        return ChatPromptTemplate.from_messages([system_prompt, human_prompt]) | self.llm
    
    def _create_kpi_generator_chain(self):
        """
        Create KPI generation chain.
        
        Constructs a LangChain pipeline that generates key performance indicators
        with Tableau-compatible calculation formulas, including support for table
        calculations and Level of Detail (LOD) expressions.
        
        Returns
        -------
        LangChain Chain
            Configured chain for KPI specification generation
            
        Notes
        -----
        Generated KPIs include:
        - Name and business description
        - Tableau calculation formula (syntactically validated)
        - Target values and formatting specifications
        - Priority ranking for dashboard placement
        
        Supports advanced Tableau formulas:
        - Aggregate functions (SUM, AVG, COUNT, etc.)
        - Table calculations (RUNNING_SUM, RANK, etc.)
        - LOD expressions (FIXED, INCLUDE, EXCLUDE)
        
        Uses meta-prompting from config.meta_prompting.system_prompts.worksheet_creator
        """
        system_prompt = SystemMessagePromptTemplate.from_template(
            self.config.meta_prompting.system_prompts.worksheet_creator + 
            """
            
            CRITICAL INSTRUCTIONS:
            1. Generate relevant KPIs based on data analysis
            2. Create Tableau-compatible calculations
            3. Prioritize KPIs by business importance
            4. Include proper formatting specifications
            5. Ensure calculations are syntactically correct for Tableau
            
            Response format (JSON Array):
            [
              {
                "name": "KPI Name",
                "description": "KPI Description",
                "calculation": "Tableau calculation formula",
                "target_value": 100.0,
                "format_string": "#,##0.0%",
                "priority": 1
              }
            ]
            """
        )
        
        human_prompt = HumanMessagePromptTemplate.from_template(
            """
            Generate KPIs for this dataset:
            
            Data Analysis: {data_insights}
            Column Details: {column_details}
            Business Goals: {business_goals}
            
            Create 3-7 relevant KPIs with proper Tableau calculations.
            """
        )
        
        return ChatPromptTemplate.from_messages([system_prompt, human_prompt]) | self.llm
    
    def _create_visualization_recommender_chain(self):
        """
        Create visualization recommendation chain.
        
        Constructs a LangChain pipeline that recommends optimal visualization types
        and configurations based on data characteristics, business goals, and KPIs.
        
        Returns
        -------
        LangChain Chain
            Configured chain for visualization recommendation generation
            
        Notes
        -----
        Recommendations include:
        - Chart type selection (bar, line, scatter, etc.)
        - Field axis mappings (x-axis, y-axis, color, size)
        - Aggregation specifications (sum, avg, count, etc.)
        - Color schemes and formatting options
        - Legend and label display settings
        
        Ensures visualizations work cohesively as a dashboard story.
        """
        system_prompt = SystemMessagePromptTemplate.from_template(
            """
            You are an expert Tableau visualization specialist. Recommend specific visualizations 
            based on data characteristics and business goals.
            
            CRITICAL INSTRUCTIONS:
            1. Recommend appropriate chart types for each data relationship
            2. Specify exact field mappings (x-axis, y-axis, color, size)
            3. Consider data types and cardinality
            4. Optimize for user understanding and insight discovery
            5. Ensure visualizations work well together in a dashboard
            
            Response format (JSON Array):
            [
              {
                "chart_type": "bar",
                "title": "Chart Title",
                "x_axis": ["field1"],
                "y_axis": ["field2"],
                "color_field": "field3",
                "size_field": null,
                "filters": [],
                "color_scheme": "tableau10",
                "show_labels": true,
                "show_legend": true,
                "aggregation_type": "sum"
              }
            ]
            """
        )
        
        human_prompt = HumanMessagePromptTemplate.from_template(
            """
            Recommend visualizations for:
            
            Dataset Schema: {dataset_schema}
            Business Goals: {business_goals}
            Data Insights: {data_insights}
            KPIs: {kpis}
            
            Create 4-8 complementary visualizations that tell a cohesive story.
            """
        )
        
        return ChatPromptTemplate.from_messages([system_prompt, human_prompt]) | self.llm
    
    async def analyze_dataset(self, request: AIAnalysisRequest) -> AIAnalysisResponse:
        """
        Perform comprehensive AI analysis of the dataset for dashboard generation.
        
        Orchestrates a multi-stage analysis pipeline:
        1. Data analysis and insights
        2. Dashboard design recommendations
        3. KPI generation with Tableau calculations
        4. Visualization recommendations
        
        Parameters
        ----------
        request : AIAnalysisRequest
            Analysis request containing dataset schema, business goals, target audience,
            and other configuration parameters
            
        Returns
        -------
        AIAnalysisResponse
            Comprehensive analysis response including:
            - Dataset insights and characteristics
            - KPI specifications with calculations
            - Visualization recommendations
            - Dashboard design recommendations
            - Layout and color scheme suggestions
            - Performance considerations
            
        Raises
        ------
        Exception
            If any stage of the analysis fails and cannot recover
            
        Notes
        -----
        Calculated fields from KPIs are automatically:
        - Extracted from KPI specifications
        - Validated for proper Tableau syntax
        - Merged with existing calculated fields in the schema
        - Passed to downstream processing
        
        The process is fully async and can be run concurrently with other async operations.
        
        Examples
        --------
        >>> analyzer = TableauDashboardAnalyzer(config)
        >>> request = AIAnalysisRequest(
        ...     dataset_schema=schema,
        ...     business_goals=["Maximize revenue", "Reduce costs"],
        ...     target_audience="Executives"
        ... )
        >>> response = await analyzer.analyze_dataset(request)
        >>> print(f"Generated {len(response.recommended_visualizations)} visualizations")
        """
        try:
            logger.info(f"Starting AI analysis for dataset: {request.dataset_schema.name}")

            # Prepare data for analysis
            column_details = self._format_column_details(request.dataset_schema.columns)

            # Step 1: Data Analysis
            logger.info("Performing data analysis...")
            data_insights = await self._run_data_analysis(
                request.dataset_schema, column_details, request.business_goals, 
                request.target_audience
            )

            # Step 2: Dashboard Design Recommendations
            logger.info("Generating dashboard design recommendations...")
            design_recommendations = await self._run_dashboard_design(
                data_insights, request.business_goals, request.target_audience,
                request.dataset_schema
            )

            # Step 3: KPI Generation
            logger.info("Generating KPI recommendations...")
            kpis = await self._run_kpi_generation(
                data_insights, column_details, request.business_goals
            )
            
            calculated_fields = []
            for kpi in kpis:
                if hasattr(kpi, "calculation") and kpi.calculation:
                    formula = kpi.calculation
                    # Lightweight validation
                    warning = None
                    if not formula.strip():
                        warning = f"KPI '{kpi.name}' has an empty calculation formula."
                    elif ("{" in formula and not formula.strip().startswith("{")):
                        warning = f"KPI '{kpi.name}' formula may be an invalid LOD calculation: {formula}"
                    elif "[" not in formula:
                        warning = f"KPI '{kpi.name}' formula does not reference any fields: {formula}"
                    if warning:
                        logger.warning(warning)
                    data_type = DataType.FLOAT if "#" in getattr(kpi, "format_string", "") else DataType.STRING
                    calculated_fields.append(
                        CalculatedFieldSpec(
                            name=kpi.name,
                            formula=formula,
                            data_type=data_type,
                            role="measure"
                        )
                    )
            # Merge with any existing calculated fields in the schema
            request.dataset_schema.calculated_fields.extend(calculated_fields)

            # Step 4: Visualization Recommendations
            logger.info("Generating visualization recommendations...")
            visualizations = await self._run_visualization_recommendations(
                request.dataset_schema, request.business_goals, data_insights, kpis
            )

            # Parse design recommendations
            layout_rec, color_rec, performance_considerations = self._parse_design_recommendations(
                design_recommendations
            )

            # Create final response
            response = AIAnalysisResponse(
                dataset_insights=data_insights.dict(),
                recommended_kpis=kpis,
                recommended_visualizations=visualizations,
                dashboard_recommendations=layout_rec,
                layout_suggestions=layout_rec,  # Same as dashboard for now
                color_scheme_recommendation=color_rec,
                performance_considerations=performance_considerations
            )

            logger.info("AI analysis completed successfully")
            return response

        except Exception as e:
            logger.error(f"AI analysis failed: {e}")
            raise

    def _format_column_details(self, columns) -> str:
        """
        Format column details for AI analysis.
        
        Converts column schema objects into a readable string representation including
        data types, unique value counts, null counts, and statistical information.
        
        Parameters
        ----------
        columns : List[ColumnSchema]
            List of column schema objects to format
            
        Returns
        -------
        str
            Formatted string representation of column details, suitable for
            inclusion in AI analysis prompts
            
        Notes
        -----
        Format includes:
        - Column name
        - Data type
        - Unique value count
        - Null count
        - Statistical measures (mean, standard deviation) if available
        
        Example output:
        ```
        - age: INTEGER, 50 unique values, 2 nulls (mean: 35.2, std: 12.5)
        - name: STRING, 200 unique values, 0 nulls
        ```
        """
        details = []
        for col in columns:
            stats_str = ""
            if col.statistics:
                stats_str = f" (mean: {col.statistics.get('mean', 'N/A')}, std: {col.statistics.get('std', 'N/A')})"
            details.append(
                f"- {col.name}: {col.data_type.value}, "
                f"{col.unique_values} unique values, "
                f"{col.null_count} nulls{stats_str}"
            )
        return "\n".join(details)
    
    async def _run_data_analysis(self, schema, column_details, business_goals, target_audience) -> DataInsights:
        """
        Execute the data analysis chain.
        
        Runs the data analyzer LangChain with dataset schema information and business
        context to generate structured insights about the data.
        
        Parameters
        ----------
        schema : DatasetSchema
            Complete dataset schema object
        column_details : str
            Formatted string representation of column details
        business_goals : List[str]
            List of business goals for analysis context
        target_audience : str
            Description of target audience for dashboard
            
        Returns
        -------
        DataInsights
            Structured data insights including characteristics, business potential,
            data quality issues, and preprocessing recommendations. Returns sensible
            defaults on failure.
            
        Notes
        -----
        Gracefully handles chain execution failures with fallback insights
        to ensure analysis pipeline continues.
        """
        try:
            result = await self.data_analyzer_chain.ainvoke({
                "dataset_name": schema.name,
                "total_rows": schema.total_rows,
                "total_columns": schema.total_columns,
                "data_quality_score": schema.data_quality_score,
                "column_details": column_details,
                "business_context": schema.business_context or "General business analysis",
                "business_goals": ", ".join(business_goals),
                "target_audience": target_audience
            })
            return result
        except Exception as e:
            logger.warning(f"Data analysis chain failed: {e}")
            # Fallback
            return DataInsights(
                data_characteristics={"total_rows": schema.total_rows, "total_columns": schema.total_columns},
                business_potential=["Analyze key metrics and trends"],
                data_quality_issues=["No specific issues identified"],
                recommended_preprocessing=["Standard data cleaning"]
            )
    
    async def _run_dashboard_design(self, data_insights, business_goals, target_audience, dataset_schema):
        """
        Execute the dashboard design recommendation chain.
        
        Generates layout, color scheme, and performance recommendations based on
        data analysis results and business context.
        
        Parameters
        ----------
        data_insights : DataInsights
            Structured insights from data analysis stage
        business_goals : List[str]
            List of business goals for design context
        target_audience : str
            Target audience description for appropriate design
        dataset_schema : DatasetSchema
            Dataset schema with row and column counts
            
        Returns
        -------
        str
            JSON-formatted string containing design recommendations including
            layout, color scheme, and performance considerations
            
        Notes
        -----
        Returns sensible defaults on failure to allow analysis to continue.
        
        Recommendations focus on:
        - Dashboard layout optimization
        - Color palette selection for audience
        - Performance best practices
        """
        try:
            result = await self.dashboard_designer_chain.ainvoke({
                "data_insights": json.dumps(data_insights.dict(), indent=2),
                "business_goals": ", ".join(business_goals),
                "target_audience": target_audience,
                "dataset_info": f"Rows: {dataset_schema.total_rows}, Columns: {dataset_schema.total_columns}"
            })
            return result.content
        except Exception as e:
            logger.warning(f"Dashboard design chain failed: {e}")
            return json.dumps({
                "layout_recommendation": {
                    "confidence_score": 0.5,
                    "reasoning": "Default grid layout recommended",
                    "alternatives": ["automatic", "free_form"]
                },
                "color_scheme_recommendation": {
                    "confidence_score": 0.7,
                    "reasoning": "Tableau10 provides good color differentiation",
                    "alternatives": ["tableau20", "category10"]
                },
                "performance_considerations": ["Limit number of marks", "Use appropriate aggregation"]
            })
    
    async def _run_kpi_generation(self, data_insights, column_details, business_goals) -> List[KPISpecification]:
        """
        Execute the KPI generation chain.
        
        Generates key performance indicators with Tableau-compatible calculation
        formulas based on data analysis and business goals.
        
        Parameters
        ----------
        data_insights : DataInsights
            Structured insights from data analysis stage
        column_details : str
            Formatted string representation of column details
        business_goals : List[str]
            List of business goals for KPI selection
            
        Returns
        -------
        List[KPISpecification]
            List of KPI specifications including names, descriptions, calculations,
            target values, formatting, and priorities. Returns minimal defaults on
            failure to ensure workflow continuity.
            
        Raises
        ------
        ValueError
            If JSON parsing of KPI data fails (caught and handled with defaults)
            
        Notes
        -----
        Generated KPIs support advanced Tableau calculations:
        - Standard aggregates (SUM, AVG, COUNT)
        - Table calculations (RUNNING_SUM, RANK, PERCENTILE)
        - LOD expressions (FIXED, INCLUDE, EXCLUDE)
        
        Format strings follow Tableau number formatting conventions.
        """
        try:
            result = await self.kpi_generator_chain.ainvoke({
                "data_insights": json.dumps(data_insights.dict(), indent=2),
                "column_details": column_details,
                "business_goals": ", ".join(business_goals)
            })
            
            kpi_data = json.loads(result.content)
            return [KPISpecification(**kpi) for kpi in kpi_data]
        except Exception as e:
            logger.warning(f"KPI generation failed: {e}")
            # Return default KPIs
            return [
                KPISpecification(
                    name="Record Count",
                    description="Total number of records",
                    calculation="COUNTD([Record Number])",
                    format_string="#,##0",
                    priority=1
                )
            ]
    
    async def _run_visualization_recommendations(self, schema, business_goals, data_insights, kpis) -> List[VisualizationSpec]:
        """
        Execute the visualization recommendation chain.
        
        Generates visualization specifications based on dataset characteristics,
        business goals, data insights, and KPIs.
        
        Parameters
        ----------
        schema : DatasetSchema
            Complete dataset schema
        business_goals : List[str]
            List of business goals for visualization context
        data_insights : DataInsights
            Structured insights from data analysis
        kpis : List[KPISpecification]
            List of generated KPI specifications
            
        Returns
        -------
        List[VisualizationSpec]
            List of visualization specifications including chart types, field
            mappings, aggregations, and formatting. Returns generated defaults on
            chain failure.
            
        Notes
        -----
        Visualizations are selected to:
        - Work together as a cohesive dashboard narrative
        - Optimize for audience understanding
        - Highlight key insights and KPIs
        - Balance data density with clarity
        
        Supports 14+ chart types:
        bar, line, area, scatter, pie, histogram, heatmap, treemap, map,
        filled_map, gantt, packed_bubble, box_plot, bullet_graph
        """
        try:
            result = await self.visualization_recommender_chain.ainvoke({
                "dataset_schema": json.dumps({
                    "columns": [col.dict() for col in schema.columns],
                    "total_rows": schema.total_rows
                }, indent=2),
                "business_goals": ", ".join(business_goals),
                "data_insights": json.dumps(data_insights.dict(), indent=2),
                "kpis": json.dumps([kpi.dict() for kpi in kpis], indent=2)
            })
            
            viz_data = json.loads(result.content)
            return [VisualizationSpec(**viz) for viz in viz_data]
        except Exception as e:
            logger.warning(f"Visualization recommendation failed: {e}")
            # Return default visualizations based on column types
            return self._generate_default_visualizations(schema)
    
    def _generate_default_visualizations(self, schema) -> List[VisualizationSpec]:
        """
        Generate default visualizations based on data types.
        
        Fallback visualization generation when AI recommendations fail. Creates
        basic but effective visualizations based on column data types and
        relationships.
        
        Parameters
        ----------
        schema : DatasetSchema
            Dataset schema used to identify column types and select appropriate
            visualizations
            
        Returns
        -------
        List[VisualizationSpec]
            List of default visualization specifications
            - Bar chart if numeric + categorical columns present
            - Scatter plot if 2+ numeric columns available
            - Additional default visualizations based on data availability
            
        Notes
        -----
        Follows these heuristics:
        - Numeric vs. Categorical: Bar chart
        - Numeric vs. Numeric: Scatter plot
        - Single metric: KPI card or trend line
        
        All defaults use sensible aggregations (sum for aggregates, avg for
        relationships, count for distributions).
        """
        visualizations = []
        numeric_cols = [col for col in schema.columns if col.data_type in [DataType.INTEGER, DataType.FLOAT]]
        categorical_cols = [col for col in schema.columns if col.data_type == DataType.CATEGORICAL]
        
        if numeric_cols and categorical_cols:
            visualizations.append(
                VisualizationSpec(
                    chart_type=VisualizationType.BAR,
                    title=f"{numeric_cols[0].name} by {categorical_cols[0].name}",
                    x_axis=[categorical_cols[0].name],
                    y_axis=[numeric_cols[0].name],
                    aggregation_type="sum"
                )
            )
        
        if len(numeric_cols) >= 2:
            visualizations.append(
                VisualizationSpec(
                    chart_type=VisualizationType.SCATTER,
                    title=f"{numeric_cols[0].name} vs {numeric_cols[1].name}",
                    x_axis=[numeric_cols[0].name],
                    y_axis=[numeric_cols[1].name],
                    aggregation_type="avg"
                )
            )
        
        return visualizations
    
    def _parse_design_recommendations(self, design_rec_str):
        """
        Parse design recommendations from AI response.
        
        Extracts structured design recommendations from AI-generated JSON response
        into typed AIRecommendation objects with confidence scores and alternatives.
        
        Parameters
        ----------
        design_rec_str : str
            JSON-formatted string containing design recommendations from AI chain
            
        Returns
        -------
        Tuple[AIRecommendation, AIRecommendation, List[str]]
            Tuple containing:
            - layout_rec (AIRecommendation): Layout recommendation with confidence
            - color_rec (AIRecommendation): Color scheme recommendation with confidence
            - performance_considerations (List[str]): List of performance tips
            
        Notes
        -----
        Gracefully handles parsing failures with sensible defaults:
        - Layout: 50% confidence, "Default layout"
        - Color: 70% confidence, "Default colors"
        - Performance: Generic optimization tips
        
        Each recommendation includes:
        - confidence_score: 0.0-1.0 indicating AI confidence
        - reasoning: Explanation of recommendation
        - alternatives: List of alternative options
        """
        try:
            data = json.loads(design_rec_str)
            
            layout_rec = AIRecommendation(
                confidence_score=data.get("layout_recommendation", {}).get("confidence_score", 0.5),
                reasoning=data.get("layout_recommendation", {}).get("reasoning", "Default layout"),
                alternatives=data.get("layout_recommendation", {}).get("alternatives", [])
            )
            
            color_rec = AIRecommendation(
                confidence_score=data.get("color_scheme_recommendation", {}).get("confidence_score", 0.7),
                reasoning=data.get("color_scheme_recommendation", {}).get("reasoning", "Default colors"),
                alternatives=data.get("color_scheme_recommendation", {}).get("alternatives", [])
            )
            
            performance_considerations = data.get("performance_considerations", [])
            
            return layout_rec, color_rec, performance_considerations
            
        except Exception as e:
            logger.warning(f"Failed to parse design recommendations: {e}")
            return (
                AIRecommendation(confidence_score=0.5, reasoning="Default recommendation", alternatives=[]),
                AIRecommendation(confidence_score=0.7, reasoning="Default color scheme", alternatives=[]),
                ["Use appropriate aggregation", "Limit data points"]
            )